{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10957b46",
   "metadata": {},
   "source": [
    "# üß† Brain Tumor Detection - Model Training Pipeline\n",
    "\n",
    "## Complete Deep Learning Training Guide\n",
    "\n",
    "This notebook contains the complete process to train a Convolutional Neural Network (CNN) for brain tumor detection from MRI scans.\n",
    "\n",
    "### Dataset Information\n",
    "- **Source**: [Kaggle Brain Tumor Detection Dataset](https://www.kaggle.com/datasets/ahmedhamada0/brain-tumor-detection)\n",
    "- **Total Images**: 7,000+ MRI scans\n",
    "- **Classes**: 4 tumor types (Glioma, Meningioma, No Tumor, Pituitary)\n",
    "- **Image Size**: 224x224 pixels\n",
    "- **Format**: JPG, PNG\n",
    "\n",
    "### Model Performance Target\n",
    "- **Accuracy**: 97%+\n",
    "- **Precision**: 96%+\n",
    "- **Recall**: 97%+\n",
    "- **F1-Score**: 96%+\n",
    "\n",
    "### Training Environment\n",
    "- Python 3.8+\n",
    "- TensorFlow 2.10.0\n",
    "- Keras 2.10.0\n",
    "- CUDA (Optional for GPU acceleration)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349e4c5e",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3cb171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Core Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, \n",
    "    ModelCheckpoint, \n",
    "    ReduceLROnPlateau,\n",
    "    TensorBoard\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Import Scikit-learn for metrics\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "# Check TensorFlow version and GPU availability\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"Python Version: {tf.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2765b7",
   "metadata": {},
   "source": [
    "## Section 2: Load and Prepare Training Data\n",
    "\n",
    "### Dataset Path Configuration\n",
    "Update the `dataset_path` variable to point to your downloaded Kaggle dataset directory.\n",
    "\n",
    "**Dataset Structure Expected:**\n",
    "```\n",
    "brain_tumor_dataset/\n",
    "‚îú‚îÄ‚îÄ Training/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ glioma_tumor/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ meningioma_tumor/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ no_tumor/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ pituitary_tumor/\n",
    "‚îî‚îÄ‚îÄ Testing/\n",
    "    ‚îú‚îÄ‚îÄ glioma_tumor/\n",
    "    ‚îú‚îÄ‚îÄ meningioma_tumor/\n",
    "    ‚îú‚îÄ‚îÄ no_tumor/\n",
    "    ‚îî‚îÄ‚îÄ pituitary_tumor/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cf8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure dataset paths\n",
    "# IMPORTANT: Update this path to your downloaded Kaggle dataset\n",
    "dataset_path = \"path/to/brain_tumor_dataset\"  # Change this to your dataset location\n",
    "\n",
    "# Verify dataset exists\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(f\"‚ö†Ô∏è Dataset path not found: {dataset_path}\")\n",
    "    print(\"Please download the dataset from: https://www.kaggle.com/datasets/ahmedhamada0/brain-tumor-detection\")\n",
    "else:\n",
    "    print(f\"‚úì Dataset found at: {dataset_path}\")\n",
    "\n",
    "# Define image parameters\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "# Classes for classification\n",
    "CLASSES = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "print(f\"\\nImage Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Number of Classes: {NUM_CLASSES}\")\n",
    "print(f\"Classes: {CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7989efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation for Training Data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # 20% validation, 80% training\n",
    ")\n",
    "\n",
    "# No augmentation for testing/validation data, only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load training data from directory\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    os.path.join(dataset_path, 'Training'),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Load validation data\n",
    "validation_data = train_datagen.flow_from_directory(\n",
    "    os.path.join(dataset_path, 'Training'),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Load testing data\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    os.path.join(dataset_path, 'Testing'),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Display data information\n",
    "print(\"Training Data Shape:\", train_data.samples)\n",
    "print(\"Validation Data Shape:\", validation_data.samples)\n",
    "print(\"Test Data Shape:\", test_data.samples)\n",
    "print(\"\\nClass Mapping:\")\n",
    "for class_name, index in train_data.class_indices.items():\n",
    "    print(f\"  {index}: {class_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbe0c52",
   "metadata": {},
   "source": [
    "## Section 3: Define the Model Architecture\n",
    "\n",
    "### CNN Architecture Overview\n",
    "We'll build a deep convolutional neural network with:\n",
    "- **Conv2D layers** for feature extraction\n",
    "- **MaxPooling layers** for dimensionality reduction\n",
    "- **Dropout layers** for regularization\n",
    "- **Dense layers** for classification\n",
    "- **Batch Normalization** for improved training stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2fd5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN Model\n",
    "model = Sequential([\n",
    "    # Block 1\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same', \n",
    "                  input_shape=(IMG_SIZE, IMG_SIZE, 3), name='conv1_1'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='conv1_2'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    # Block 2\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2_1'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2_2'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    # Block 3\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='conv3_1'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='conv3_2'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2), name='pool3'),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    # Block 4\n",
    "    layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='conv4_1'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='conv4_2'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2), name='pool4'),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    # Flattening and Dense layers\n",
    "    layers.Flatten(name='flatten'),\n",
    "    layers.Dense(512, activation='relu', name='dense1'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation='relu', name='dense2'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(NUM_CLASSES, activation='softmax', name='output')\n",
    "])\n",
    "\n",
    "# Display model summary\n",
    "print(\"Model Architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c0dc3",
   "metadata": {},
   "source": [
    "## Section 4: Compile the Model\n",
    "\n",
    "### Compilation Details\n",
    "- **Optimizer**: Adam (adaptive learning rate)\n",
    "- **Loss Function**: Categorical Crossentropy (for multi-class classification)\n",
    "- **Metrics**: Accuracy for monitoring model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3437f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "print(f\"Loss Function: Categorical Crossentropy\")\n",
    "print(f\"Metrics: Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7b2c07",
   "metadata": {},
   "source": [
    "## Section 5: Train the Model\n",
    "\n",
    "### Training Strategy\n",
    "- **Epochs**: 50 (with early stopping)\n",
    "- **Batch Size**: 32\n",
    "- **Early Stopping**: Stop if validation accuracy doesn't improve for 5 epochs\n",
    "- **Model Checkpoint**: Save best model during training\n",
    "- **Learning Rate Reduction**: Reduce learning rate if loss plateaus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0495d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callback functions for training\n",
    "callbacks = [\n",
    "    # Early stopping to prevent overfitting\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Save best model\n",
    "    ModelCheckpoint(\n",
    "        'best_brain_tumor_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate on plateau\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # TensorBoard for visualization\n",
    "    TensorBoard(log_dir='./logs', histogram_freq=1)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "print(\"This may take several hours depending on your hardware.\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_data,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    steps_per_epoch=train_data.samples // BATCH_SIZE,\n",
    "    validation_steps=validation_data.samples // BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a4e84",
   "metadata": {},
   "source": [
    "## Section 6: Evaluate Model Performance\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Accuracy**: Overall correct predictions\n",
    "- **Precision**: Correct positive predictions out of all positive predictions\n",
    "- **Recall**: Correct positive predictions out of all actual positives\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **Confusion Matrix**: True/False positives and negatives\n",
    "- **ROC Curve**: Receiver Operating Characteristic curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331f771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0].set_title('Model Accuracy Over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[1].set_title('Model Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history plot saved as 'training_history.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbb657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "print(\"Evaluating model on test set...\\n\")\n",
    "\n",
    "# Get predictions\n",
    "test_loss, test_accuracy = model.evaluate(test_data, verbose=1)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Test Results:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Get predictions for detailed metrics\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in test_data:\n",
    "    predictions = model.predict(images, verbose=0)\n",
    "    y_true.extend(np.argmax(labels, axis=1))\n",
    "    y_pred.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\nDetailed Performance Metrics:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Accuracy:  {accuracy*100:.2f}%\")\n",
    "print(f\"Precision: {precision*100:.2f}%\")\n",
    "print(f\"Recall:    {recall*100:.2f}%\")\n",
    "print(f\"F1-Score:  {f1*100:.2f}%\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54805a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_true, y_pred, target_names=CLASSES, digits=4))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=CLASSES, yticklabels=CLASSES, cbar=True)\n",
    "plt.title('Confusion Matrix - Brain Tumor Detection', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix plot saved as 'confusion_matrix.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bddef45",
   "metadata": {},
   "source": [
    "## Section 7: Save the Trained Model\n",
    "\n",
    "### Model Saving Options\n",
    "1. **H5 Format**: Keras native format (used in main application)\n",
    "2. **SavedModel Format**: TensorFlow's recommended format\n",
    "3. **ONNX Format**: For cross-platform compatibility (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model in H5 format (compatible with Flask app)\n",
    "output_path = \"../static/models/model_2.h5\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model.save(output_path)\n",
    "print(f\"‚úì Model saved to: {output_path}\")\n",
    "\n",
    "# Verify model was saved\n",
    "if os.path.exists(output_path):\n",
    "    file_size = os.path.getsize(output_path) / (1024 * 1024)  # Size in MB\n",
    "    print(f\"‚úì File size: {file_size:.2f} MB\")\n",
    "else:\n",
    "    print(f\"‚úó Error: Model file not found at {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f84b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save model in SavedModel format (TensorFlow format)\n",
    "savedmodel_path = \"../static/models/brain_tumor_savedmodel\"\n",
    "\n",
    "model.save(savedmodel_path)\n",
    "print(f\"‚úì Model also saved in SavedModel format: {savedmodel_path}\")\n",
    "\n",
    "# Optional: Save model information and metrics\n",
    "model_info = {\n",
    "    'model_name': 'Brain Tumor Detection CNN',\n",
    "    'input_shape': (IMG_SIZE, IMG_SIZE, 3),\n",
    "    'output_classes': NUM_CLASSES,\n",
    "    'classes': CLASSES,\n",
    "    'test_accuracy': float(test_accuracy),\n",
    "    'test_precision': float(precision),\n",
    "    'test_recall': float(recall),\n",
    "    'test_f1_score': float(f1),\n",
    "    'total_parameters': model.count_params(),\n",
    "    'training_date': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save model info as JSON\n",
    "import json\n",
    "with open('model_info.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=4)\n",
    "\n",
    "print(\"\\n‚úì Model information saved to: model_info.json\")\n",
    "print(\"\\nModel Summary:\")\n",
    "print(json.dumps(model_info, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2191b837",
   "metadata": {},
   "source": [
    "## Next Steps After Training\n",
    "\n",
    "### 1. **Load Trained Model for Inference**\n",
    "```python\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('model_2.h5')\n",
    "\n",
    "# Make predictions on new images\n",
    "predictions = model.predict(new_images)\n",
    "```\n",
    "\n",
    "### 2. **Integrate with Flask Application**\n",
    "- Copy `model_2.h5` to `../static/models/` directory\n",
    "- The Flask app will automatically load the model on first request\n",
    "\n",
    "### 3. **Model Deployment**\n",
    "- Use the saved model for production inference\n",
    "- Consider containerization with Docker\n",
    "- Deploy on cloud platforms (AWS, Google Cloud, Azure)\n",
    "\n",
    "### 4. **Model Monitoring & Retraining**\n",
    "- Monitor model performance on new data\n",
    "- Retrain periodically with fresh data\n",
    "- Track metrics over time\n",
    "\n",
    "### 5. **Further Improvements**\n",
    "- **Transfer Learning**: Use pre-trained models (VGG16, ResNet, Inception)\n",
    "- **Ensemble Methods**: Combine multiple models for better accuracy\n",
    "- **Data Augmentation**: Increase training data with augmentation techniques\n",
    "- **Hyperparameter Tuning**: Use GridSearchCV or Bayesian Optimization\n",
    "\n",
    "---\n",
    "\n",
    "## Training Tips and Best Practices\n",
    "\n",
    "### 1. **Data Quality**\n",
    "- Ensure balanced class distribution\n",
    "- Remove corrupted or low-quality images\n",
    "- Normalize pixel values (0-1 range)\n",
    "\n",
    "### 2. **Model Architecture**\n",
    "- Use Batch Normalization for stability\n",
    "- Apply Dropout for regularization\n",
    "- Use appropriate activation functions\n",
    "\n",
    "### 3. **Training Configuration**\n",
    "- Use validation data to monitor overfitting\n",
    "- Implement early stopping\n",
    "- Use learning rate scheduling\n",
    "- Apply L1/L2 regularization if needed\n",
    "\n",
    "### 4. **Hyperparameter Tuning**\n",
    "- Learning rate: 0.0001 to 0.01\n",
    "- Batch size: 16, 32, or 64\n",
    "- Dropout rate: 0.3 to 0.5\n",
    "- Optimizer: Adam, SGD with momentum\n",
    "\n",
    "### 5. **Handling Imbalanced Data**\n",
    "```python\n",
    "# Calculate class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "# Use in training\n",
    "model.fit(X_train, y_train, \n",
    "          class_weight=dict(enumerate(class_weights)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting Guide\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| **Out of Memory (OOM)** | Reduce batch size, use gradient accumulation |\n",
    "| **Overfitting** | Increase dropout, use regularization, more data augmentation |\n",
    "| **Underfitting** | Increase model complexity, reduce dropout, train longer |\n",
    "| **Slow Training** | Use GPU acceleration, reduce image size, smaller batch size |\n",
    "| **Poor Accuracy** | Check data quality, adjust hyperparameters, increase training data |\n",
    "| **NaN Values** | Check learning rate, normalize data, check for bad data |\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [TensorFlow Documentation](https://www.tensorflow.org/docs)\n",
    "- [Keras API Reference](https://keras.io/api/)\n",
    "- [Deep Learning Best Practices](https://cs231n.github.io/)\n",
    "- [Kaggle Brain Tumor Dataset](https://www.kaggle.com/datasets/ahmedhamada0/brain-tumor-detection)\n",
    "\n",
    "---\n",
    "\n",
    "**Training Complete!** üéâ\n",
    "\n",
    "Your model is now ready for inference in the Flask web application.\n",
    "Replace the model file in `static/models/model_2.h5` with your newly trained model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
